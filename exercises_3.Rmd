---
title: "exercises_3"
author: "Bas Machielsen"
date: '2022-07-13'
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Exercise 3



```{python}

import pandas as pd
import tensorflow as tf
import numpy as np
from tensorflow import keras
from tensorflow.keras import layers
from keras.models import Sequential
from keras.layers import Conv2D
from keras.layers import MaxPooling2D
from keras.layers import Dense
from keras.layers import Flatten
from keras.optimizers import SGD

from sklearn import preprocessing
from sklearn.model_selection import train_test_split

from keras.datasets import mnist

(X_train, y_train), (X_test, y_test)  = mnist.load_data()

X_train.shape
```



- You have already used Keras to create a first deep learning model for the MNIST data. Build on that code to create a convolutional neural network.

```{python}
model = Sequential()

# Processing
model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(32, 28, 28, 1)))
model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
model.add(MaxPooling2D((2, 2)))

model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
model.add(MaxPooling2D((2, 2)))

# Classification
model.add(Flatten())
model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))
model.add(Dense(10, activation='softmax'))

model.compile(
    loss= tf.keras.losses.SparseCategoricalCrossentropy(), # The loss function that is being minimized
    optimizer=tf.keras.optimizers.SGD(),
    metrics=['accuracy', 'mse'] 
)


```


```{python}
model.fit(
    X_train, # Input training data
    y_train, # Output training data
    epochs=5, # Amount of iterations we want to train for
    verbose=1 # Amount of detail you want shown in terminal while training
)

```


```{python}
for layer in model.layers:
    X = layer(X)
    print(layer.__class__.__name__,'output shape:\t', X.shape)

```
- Experiment with adding a pooling layer


- Apply dropout to one of the layers. Use 0.25 dropout rate.


- Try few configurations of the hyper parameters (optimization procedure, batch size, number of epochs, etc.).
